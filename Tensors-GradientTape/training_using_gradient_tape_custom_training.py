# -*- coding: utf-8 -*-
"""training_using_gradient_tape_custom_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14OXmw606TbtES_76DejvDJiMcOom2Jlr

Importing all required modules

Using custom trainig method on simple dataset, which has simple relationship of y = 2x-1
"""

import numpy as np
import tensorflow as tf
import random

"""Creating xs and ys for training. Here we are using simple data, and y = 2x-1. Machine learnig will figure out the Wx+b
for us, here W should be equal to 2 or closer to 2 AND b should be equal to -1 or closer to -1.
"""

#creating training data 
xs = np.array([-1.0, 0.0 , 1.0, 2.0, 3.0, 4.0], dtype = float)
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype = float)

# ys approx 2x -1

"""initializing w and b, which are learning paramters, their values will be manipulated by learing algorithm to match y_true to y_pred."""

# creating training parameters
w = tf.Variable(initial_value = random.random(), trainable = True)
b = tf.Variable(initial_value = random.random(), trainable= True)

"""Defining a simple custom loss function for training. Here we are using |y_true - y_pred| as our loss function for simplicity."""

def my_loss(y_true , y_pred):
  return tf.abs(y_true - y_pred)

learning_rate = 0.001 # its a hyper-paramters whose value you can change, please feel free to play with it.

"""defining a training function,  which takes training data and it will return w and b values. GradientTape is used to calcualte the gradients of the loss fuction with respect to the trainable variables, and then those values are updated at the end."""

def fit_data(x_train, y_train):
  with tf.GradientTape(persistent = True) as tape:
    # calculating prediction
    y_pred = w * x_train + b
    # calculating loss
    reg_loss = my_loss(y_train, y_pred)
  # now calculating gradients with respect to w and b
  w_gradient = tape.gradient(reg_loss, w)
  b_gradient = tape.gradient(reg_loss, b)

  # now updating paramters w and b
  w.assign_sub(w_gradient*learning_rate)
  b.assign_sub(b_gradient*learning_rate)

"""A for loop is used to run for 1000 iterations, which will call training function and it will update learning variables on each iteration. Please feel free to play with this hyperparameter """

num_iters = 1000
for _ in range(num_iters):
  fit_data(xs, ys)

print(f'y = {w.numpy()}x + {b.numpy()}')